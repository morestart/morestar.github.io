{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pycharm-a89d98d0",
   "display_name": "PyCharm (song)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 张量和操作\n",
    "\n",
    "### 导入TF\n",
    "\n",
    "在开始前，先导入tf2，在tf2中，动态图默认开启，这使得tf2的前端更具交互性，我们将在后面详细讨论。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "source": [
    "## 张量\n",
    "张量本质上就是一个多维矩阵,类似于Numpy的`ndarray`类型,`tf.Tensor`类型拥有数据类型和维度.另外`tf.Tensor`类型数据可以在GPU中进行加速,TF提供了很多计算和创造tensor的库,这些操作可以自动的转换到原生的Python类型,例如:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(3, shape=(), dtype=int32)\ntf.Tensor([4 6], shape=(2,), dtype=int32)\ntf.Tensor(25, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(13, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.add(1, 2))\n",
    "print(tf.add([1, 2], [3, 4]))\n",
    "print(tf.square(5))\n",
    "print(tf.reduce_sum([1, 2, 3]))\n",
    "\n",
    "print(tf.square(2) + tf.square(3))"
   ]
  },
  {
   "source": [
    "每一个`tf.Tensor`都有维度和数据类型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([[2 3]], shape=(1, 2), dtype=int32)\n(1, 2)\n<dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "x = tf.matmul([[1]], [[2, 3]])\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(x.dtype)"
   ]
  },
  {
   "source": [
    "numpy数组和tf.Tensor最明显的区别是:\n",
    "\n",
    "1. 张量能否使用GPU等加速\n",
    "\n",
    "2. 张量是不变的 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Numpy兼容"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "在`tf.Tensor`和Numpy`ndarray`中转换是容易的:\n",
    "    - TF会自动将ndarray转换为Tensor\n",
    "    - Numpy操作会自动将Tensor转换为ndarray\n",
    "\n",
    "张量使用它们的`.numpy()`方法显式转换为NumPy ndarray.由于数组和`tf.tensor`共享底层内存表示。但是，共享底层表示并不总是可能的，因为`tf.tensor`可能托管在GPU内存中，而NumPy数组始终由主机内存支持，并且转换涉及从GPU到主机内存的副本。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf自动转换ndarray到tensor\ntf.Tensor(\n[[42. 42. 42.]\n [42. 42. 42.]\n [42. 42. 42.]], shape=(3, 3), dtype=float64)\nnumpy自动转换tensor到ndarray\n[[43. 43. 43.]\n [43. 43. 43.]\n [43. 43. 43.]]\n使用.numpy函数将tensor转换为numpy array\n[[42. 42. 42.]\n [42. 42. 42.]\n [42. 42. 42.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ndarray = np.ones([3, 3])\n",
    "print('tf自动转换ndarray到tensor')\n",
    "tensor = tf.multiply(ndarray, 42)\n",
    "print(tensor)\n",
    "\n",
    "print('numpy自动转换tensor到ndarray')\n",
    "print(np.add(tensor, 1))\n",
    "\n",
    "print('使用.numpy函数将tensor转换为numpy array')\n",
    "print(tensor.numpy())"
   ]
  },
  {
   "source": [
    "## GPU加速\n",
    "许多tf操作可以通过GPU来加速计算,在没有任何注释的情况下，如果必要的话,TensorFlow会自动决定是否使用GPU还是CPU来执行在CPU和GPU内存之间复制张量的操作。操作产生的张量通常由执行操作的设备的内存支持，例如："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU 可用\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\nTensor on GPU #0: \nTrue\n"
     ]
    }
   ],
   "source": [
    "# 均匀分布\n",
    "x = tf.random.uniform([3,3])\n",
    "print('GPU 可用')\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))\n",
    "\n",
    "print('Tensor on GPU #0: ')\n",
    "print(x.device.endswith('GPU:0'))"
   ]
  },
  {
   "source": [
    "## 设备名称\n",
    "\n",
    "`Tensor.device`属性提供了承载张量内容的设备的完整的名称和地址.此名称对很多细节进行了编码,像网络地址标识符以及在该主机上执行的设备,这是tf分布式程序所必需的.这个字符串使用`GPU:<N>`结尾,如果tensor放置在了`N`-th GPU上,例如:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 显式设备指定\n",
    "\n",
    "在tf中,如果没有指定设备,tf会自动决定使用哪个设备进行操作,如果要指定设备进行操作的话,可以使用`tf.device`进行显式设备指定"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "On CPU:\n",
      "10 loops: 68.30ms\n",
      "On GPU:\n",
      "10 loops: 211.94ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def time_matmul(x):\n",
    "    start = time.time()\n",
    "    for loop in range(10):\n",
    "        tf.matmul(x, x)\n",
    "    \n",
    "    result = time.time()-start\n",
    "    print('10 loops: {:0.2f}ms'.format(1000*result))\n",
    "\n",
    "print('On CPU:')\n",
    "\n",
    "with tf.device(\"CPU:0\"):\n",
    "    x = tf.random.uniform([1000, 1000])\n",
    "    assert x.device.endswith('CPU:0')\n",
    "    time_matmul(x)\n",
    "\n",
    "if tf.config.experimental.list_physical_devices('GPU'):\n",
    "    print('On GPU:')\n",
    "    with tf.device('GPU:0'):\n",
    "        x = tf.random.uniform([1000, 1000])\n",
    "        assert x.device.endswith('GPU:0')\n",
    "        time_matmul(x)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}